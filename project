# âœ… STEP 1: Install Dependencies
!apt install tesseract-ocr -y
!pip install pymupdf faiss-cpu sentence-transformers pytesseract Pillow groq

# âœ… STEP 2: Upload PDF
from google.colab import files
import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import io

uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]

# âœ… STEP 3: Extract Text with OCR Fallback
def extract_text_from_pdf_with_ocr(file_path):
    doc = fitz.open(file_path)
    full_text = ""

    for page_num in range(len(doc)):
        page = doc[page_num]
        text = page.get_text()

        if not text.strip():  # No text = scanned image
            print(f"OCR applied on page {page_num + 1}")
            pix = page.get_pixmap(dpi=300)
            img = Image.open(io.BytesIO(pix.tobytes("png")))
            text = pytesseract.image_to_string(img)

        full_text += text + "\n"

    return full_text

text = extract_text_from_pdf_with_ocr(pdf_path)
print("âœ… Extracted Text Preview:\n", text[:500])

# âœ… STEP 4: Chunk Text
def chunk_text(text, max_words=100):
    words = text.split()
    return [" ".join(words[i:i + max_words]) for i in range(0, len(words), max_words)]

chunks = chunk_text(text)
print(f"âœ… Total Chunks: {len(chunks)}")
print("ðŸ”¹ First Chunk:\n", chunks[0])

# âœ… STEP 5: Generate Embeddings
from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(chunks)
print("âœ… Embeddings Shape:", embeddings.shape)

# âœ… STEP 6: Build FAISS Index
import faiss

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

def search_faiss(query, model, index, chunks, top_k=5):
    query_vec = model.encode([query])
    D, I = index.search(np.array(query_vec), top_k)
    return [chunks[i] for i in I[0]]

# âœ… STEP 7: Groq LLM via SDK
from groq import Groq
import os

os.environ["GROQ_API_KEY"] = input("ðŸ”‘ Enter your Groq API Key: ")
client = Groq(api_key=os.environ["GROQ_API_KEY"])

def query_groq_llm(context_chunks, question):
    context = "\n\n".join(context_chunks)
    prompt = f"""You are a helpful assistant. Use the following document context to answer the question.\n\nContext:\n{context}\n\nQuestion: {question}\nAnswer:"""

    chat_completion = client.chat.completions.create(
        model="llama3-8b-8192",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2
    )

    return chat_completion.choices[0].message.content

# âœ… STEP 8: Ask a Question
query = input("ðŸ’¬ Enter your question: ")
top_chunks = search_faiss(query, model, index, chunks)
answer = query_groq_llm(top_chunks, query)

print("\nðŸ“˜ Answer:\n", answer)
